# A Survey: Spatiotemporal Consistency in Video Generation

This project lists the files related to the spatiotemporal consistency in video generation.

## üî• Updates

- [12/2025] Create üî•[ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊó∂Á©∫‰∏ÄËá¥ÊÄßÔºöÂÖ≥‰∫éËßÜÈ¢ëÁîüÊàê‰∏≠Êó∂Á©∫‰∏ÄËá¥ÊÄßÁöÑÁªºËø∞ --- Yin-Z-Y/A-Survey-Spatiotemporal-Consistency-in-Video-Generation: A survey on spatiotemporal consistency in video generation](https://github.com/Yin-Z-Y/A-Survey-Spatiotemporal-Consistency-in-Video-Generation)üî• repository


## üì£ Overview

### A Survey: Spatiotemporal Consistency in Video Generation

Video generation aims to produce temporally coherent sequences of visual frames, representing a pivotal advancement in Artificial Intelligence Generated Content (AIGC). Compared to static image generation, video generation poses unique challenges: it demands not only high-quality individual frames but also strong temporal coherence to ensure consistency throughout the spatiotemporal sequence. Although research addressing spatiotemporal consistency in video generation has increased in recent years, systematic reviews focusing on this core issue remain relatively scarce. To fill this gap, this paper views the video generation task as a sequential sampling process from a high-dimensional spatiotemporal distribution, and further discusses spatiotemporal consistency. We provide a systematic review of the latest advancements in the field. The content spans multiple dimensions including generation models, feature representations, generation frameworks, post-processing techniques, training strategies, benchmarks and evaluation metrics, with a particular focus on the mechanisms and effectiveness of various methods in maintaining spatiotemporal consistency. Finally, this paper explores future research directions and potential challenges in this field, aiming to provide valuable insights for advancing video generation technology.

## üéì Papers

### Survey

- [2022/09] Diffusion Models: A Comprehensive Survey of Methods and Applications | [[Paper]](https://arxiv.org/abs/2209.00796)
- [2023/01] Video stabilization: A comprehensive survey | [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S092523122201270X)
- [2023/10] A Survey on Video Diffusion Models | [[Paper]](https://arxiv.org/abs/2310.10647)
- [2023/11] A Survey of AI Text-to-Image and AI Text-to-Video Generators | [[Paper]](https://arxiv.org/abs/2311.06329)
- [2023/11] Video Frame Interpolation: A Comprehensive Survey | [[Paper]](https://dl.acm.org/doi/abs/10.1145/3556544)
- [2024/02] Video generation models as world simulators | [[Paper]](https://openai.com/research/video-generation-models-as-world-simulators)
- [2024/02] Sora: A review on background, technology, limitations, and opportunities of large vision models | [[Paper]](https://arxiv.org/abs/2403.16407)
- [2024/03] A survey on long video generation: Challenges, methods, and prospects | [[Paper]](https://arxiv.org/abs/2403.16407)
- [2024/03] Sora as an agi world model? a complete survey on text-to-video generation | [[Paper]](https://arxiv.org/abs/2403.05131)
- [2024/04] A survey on generative ai and llm for video generation, understanding, and streaming | [[Paper]](https://arxiv.org/abs/2404.16038)
- [2024/05] From Sora What We Can See: A Survey of Text-to-Video Generation | [[Paper]](https://arxiv.org/abs/2405.10674)
- [2024/05] Video Diffusion Models: A Survey | [[Paper]](https://arxiv.org/abs/2405.03150)
- [2024/07] A comprehensive survey on human video generation: Challenges, methods, and insights | [[Paper]](https://arxiv.org/abs/2407.08428)
- [2024/11] Autoregressive Models in Vision: A Survey | [[Paper]](https://arxiv.org/abs/2411.05902)
- [2025/04] A Survey of Interactive Generative Video | [[Paper]](https://arxiv.org/abs/2504.21853)
- [2025/04] Survey of Video Diffusion Models: Foundations, Implementations, and Applications | [[Paper]](https://arxiv.org/abs/2504.16081)
- [2025/06] AceVFI: A Comprehensive Survey of Advances in Video Frame Interpolation | [[Paper]](https://arxiv.org/abs/2506.01061)
- [2025/07] A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality | [[Paper]](https://arxiv.org/abs/2507.07202)
- [2025/07] Controllable Video Generation: A Survey | [[Paper]](https://arxiv.org/abs/2507.16869)



### Generation models

- [2018/05] Information Constraints on Auto-Encoding Variational Bayes | [[Paper]](https://arxiv.org/abs/1805.08672)
- [2020/10] An image is worth 16x16 words: Transformers for image recognition at scale | [[Paper]](https://arxiv.org/abs/2010.11929)
- [2021/11] N√úWA: Visual Synthesis Pre-training for Neural visUal World creAtion | [[Paper]](https://arxiv.org/abs/2111.12417)

- [2022/05] Cogvideo: Large-scale pretraining for text-to-video generation via transformers | [[Paper]](https://arxiv.org/abs/2205.15868)
- [2022/10] Flow Matching for Generative Modeling  | [[Paper]](https://arxiv.org/abs/2210.02747)
- [2022/11] Latent video diffusion models for high-fidelity long video generation | [[Paper]](https://arxiv.org/abs/2211.13221)

- [2023/03] PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow | [[Paper]](https://arxiv.org/abs/2303.02595)
- [2023/04] Align your latents: High-resolution video synthesis with latent diffusion models | [[Paper]](https://arxiv.org/abs/2304.08818)
- [2024/01] Scalable Diffusion Models with Transformers | [[Paper]](https://ieeexplore.ieee.org/document/10377858)
- [2024/09] Od-vae: An omni-dimensional video compressor for improving latent video diffusion model | [[Paper]](https://arxiv.org/abs/2409.01199)
- [2025/02] Flashvideo: Flowing fidelity to detail for efficient high-resolution video generation | [[Paper]](https://arxiv.org/abs/2502.05179)



### Feature Representations

- [2022/03] Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training  | [[Paper]](https://arxiv.org/abs/2203.12602)
- [2022/09] Make-a-video: Text-to-video generation without text-video data | [[Paper]](https://arxiv.org/abs/2209.14792)
- [2022/10] Phenaki: Variable Length Video Generation From Open Domain Textual Description | [[Paper]](https://arxiv.org/abs/2210.02399)
- [2022/11] Latent video diffusion models for high-fidelity long video generation | [[Paper]](https://arxiv.org/abs/2211.13221)
- [2023/02] Structure and Content-Guided Video Synthesis with Diffusion Models | [[Paper]](https://arxiv.org/abs/2302.03011)
- [2023/06] SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs | [[Paper]](https://arxiv.org/abs/2306.17842)
- [2023/12] Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation | [[Paper]](https://arxiv.org/abs/2312.04483)
- [2024/05] Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing | [[Paper]](https://arxiv.org/abs/2405.04496)
- [2024/06] OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation | [[Paper]](https://arxiv.org/abs/2406.09399)
- [2024/07] FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention | [[Paper]](https://arxiv.org/abs/2407.19918)
- [2024/08] Cogvideox: Text-to-video diffusion models with an expert transformer | [[Paper]](https://arxiv.org/abs/2408.06072)
- [2024/08] xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations | [[Paper]](https://arxiv.org/abs/2408.12590)
- [2024/10] Larp: Tokenizing videos with a learned autoregressive generative prior | [[Paper]](https://arxiv.org/abs/2410.21264)
- [2024/10] FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality | [[Paper]](https://arxiv.org/abs/2410.19355)
- [2024/10] Accelerating Diffusion Transformers with Token-wise Feature Caching | [[Paper]](https://arxiv.org/abs/2410.05317)
- [2024/11] Improved Video VAE for Latent Video Diffusion Model | [[Paper]](https://arxiv.org/abs/2411.06449)
- [2024/11] WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model | [[Paper]](https://arxiv.org/abs/2411.17459)
- [2024/12] Hunyuanvideo: A systematic framework for large video generative models | [[Paper]](https://arxiv.org/abs/2412.03603)
- [2024/12] DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models | [[Paper]](https://arxiv.org/abs/2412.04446)
- [2024/12] VidTwin: Video VAE with Decoupled Structure and Dynamics | [[Paper]](https://arxiv.org/abs/2412.17726)
- [2024/12] Large Motion Video Autoencoding with Cross-modal Video VAE | [[Paper]](https://arxiv.org/abs/2412.17805)
- [2024/12] SweetTokenizer: Semantic-Aware Spatial-Temporal Tokenizer for Compact Visual Discretization | [[Paper]](https://arxiv.org/abs/22412.10443)
- [2025/03] Wan: Open and advanced large-scale video generative models | [[Paper]](https://arxiv.org/abs/2503.20314)
- [2025/03] Rethinking video tokenization: A conditioned diffusion-based approach | [[Paper]](https://arxiv.org/abs/2503.03708)
- [2025/03] Improving autoregressive image generation through coarse-to-fine token prediction | [[Paper]](https://arxiv.org/abs/2503.16194)
- [2025/03] Bridging continuous and discrete tokens for autoregressive visual generation | [[Paper]](https://arxiv.org/abs/2503.16430)
- [205/03] CODA: Repurposing Continuous VAEs for Discrete Tokenization | [[Paper]](https://arxiv.org/abs/2503.17760)
- [2025/03] LongDiff: Training-Free Long Video Generation in One Go | [[Paper]](https://arxiv.org/abs/2503.18150)
- [2025/04] Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens | [[Paper]](https://arxiv.org/abs/2504.14666)
- [2025/04] Reasoning physical video generation with diffusion timestep tokens via reinforcement learning | [[Paper]](https://arxiv.org/abs/2504.15932)
- [2025/07] FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion | [[Paper]](https://arxiv.org/abs/2507.00162)

### Generation Frameworks

- [2022/11] EDICT: Exact Diffusion Inversion via Coupled Transformations | [[Paper]](https://arxiv.org/abs/2211.12446)

- [2022/11] MagicVideo: Efficient Video Generation With Latent Diffusion Models | [[Paper]](https://arxiv.org/abs/2211.11018)

- [2022/12] Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation | [[Paper]](https://arxiv.org/abs/2212.11565)

- [2023/03] Conditional Image-to-Video Generation with Latent Flow Diffusion Models | [[Paper]](https://arxiv.org/abs/2303.13744)

- [2023/03] Nuwa-xl: Diffusion over diffusion for extremely long video generation | [[Paper]](https://arxiv.org/abs/2303.12346)

- [2023/03] Videofusion: Decomposed diffusion models for high-quality video generation | [[Paper]](https://arxiv.org/abs/2303.08320)

- [2023/04] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models | [[Paper]](https://arxiv.org/abs/2304.08818)

- [2023/04] Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models | [[Paper]](https://arxiv.org/abs/2304.12526)

- [2023/05] LEO: Generative Latent Image Animator for Human Video Synthesis | [[Paper]](https://arxiv.org/abs/2305.03989)

- [2023/05] Control-A-Video: Controllable Text-to-Video Diffusion Models with Motion Prior and Reward Feedback Learning | [[Paper]](https://arxiv.org/abs/2305.13840)

- [2023/06] Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation | [[Paper]](https://arxiv.org/abs/2306.07954)

- [2023/08] Modelscope text-to-video technical report | [[Paper]](https://arxiv.org/abs/2308.06571)

- [2023/10]  DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors | [[Paper]](https://arxiv.org/abs/2310.12190)

- [2023/10] FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling | [[Paper]](https://arxiv.org/abs/2310.15169)

- [2023/11] I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models | [[Paper]](https://arxiv.org/abs/2311.04145)

- [2023/11] Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets | [[Paper]](https://arxiv.org/abs/2311.15127)

- [2023/12] FreeInit: Bridging Initialization Gap in Video Diffusion Models | [[Paper]](https://arxiv.org/abs/2312.07537)

- [2024/01] Lumiere: A Space-Time Diffusion Model for Video Generation | [[Paper]](https://arxiv.org/abs/2401.12945)

- [2024/03] Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition | [[Paper]](https://arxiv.org/abs/2403.14148)

- [2024/05] Talc: Time-aligned captions for multi-scene text-to-video generation | [[Paper]](https://arxiv.org/abs/2405.04682)

- [2024/06] Cono: Consistency noise injection for tuning-free long video diffusion | [[Paper]](https://arxiv.org/abs/2406.05082)

- [2024/07] T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation | [[Paper]](https://arxiv.org/abs/2407.14505)

- [2024/08] Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data | [[Paper]](https://arxiv.org/abs/2408.10119)

- [2024/09] Emu3: Next-token prediction is all you need  | [[Paper]](https://arxiv.org/abs/2409.18869)

- [2024/10] Real-time 3D-aware Portrait Video Relighting | [[Paper]](https://arxiv.org/abs/2410.18355)

- [2024/10] Framer: Interactive frame interpolation | [[Paper]](https://arxiv.org/abs/2410.18978)

- [2024/10] Mardini: Masked autoregressive diffusion for video generation at scale | [[Paper]](https://arxiv.org/abs/2410.20280)

- [2024/10] Loong: Generating minute-level long videos with autoregressive language models | [[Paper]](https://arxiv.org/abs/2410.02757)

- [2024/10] Diffusion forcing: Next-token prediction meets full-sequence diffusion | [[Paper]](https://arxiv.org/abs/2407.01392)

- [2024/11] Golden noise for diffusion models: A learning framework | [[Paper]](https://arxiv.org/abs/2411.09502)

- [2024/11] Gamegen-x: Interactive open-world game video generation | [[Paper]](https://arxiv.org/abs/2411.00769)

- [2024/12] Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via Self-Reflection | [[Paper]](https://arxiv.org/abs/2412.10891)

- [2024/12] Autoregressive Video Generation without Vector Quantization | [[Paper]](https://arxiv.org/abs/2412.14169)

- [2024/12] From Slow Bidirectional to Fast Autoregressive Video Diffusion Models | [[Paper]](https://arxiv.org/abs/2412.07772)

- [2024/12] Mind the Time: Temporally-Controlled Multi-Event Video Generation | [[Paper]](https://arxiv.org/abs/2412.05263)

- [2025/01] Ltx-video: Realtime video latent diffusion | [[Paper]](https://arxiv.org/abs/2501.00103)

- [2025/02] Next block prediction: Video generation via semi-autoregressive modeling | [[Paper]](https://arxiv.org/abs/2502.07737)

- [2025/04] SkyReels-V2: Infinite-length Film Generative Model| [[Paper]](https://arxiv.org/abs/2504.13074)

- [2025/06] FastInit: Fast Noise Initialization for Temporally Consistent Video Generation | [[Paper]](https://arxiv.org/abs/2506.16119)
  [2025/06] EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning  | [[Paper]](https://arxiv.org/abs/2509.20360)

- [2025/07] Back to the Features: DINO as a Foundation for Video World Models | [[Paper]](https://arxiv.org/abs/2507.19468)

- [2025/08] Phased One-Step Adversarial Equilibrium for Video Diffusion Models | [[Paper]](https://arxiv.org/abs/2508.21019)

- [2025/11] Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model | [[Paper]](https://arxiv.org/abs/2511.23429)

- [2025/11] HunyuanVideo 1.5 Technical Report | [[Paper]](https://arxiv.org/abs/2511.18870)

  

### Post-processing Techniques

* [2017/11] Video Enhancement with Task-Oriented Flow | [[Paper]](https://arxiv.org/abs/1711.09078)
* [2018/03] Context-aware Synthesis for Video Frame Interpolation | [[Paper]](https://arxiv.org/abs/1803.10967)
* [2020/06] Learning Video Stabilization Using Optical Flow | [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Learning_Video_Stabilization_Using_Optical_Flow_CVPR_2020_paper.pdf)
* [2022/01] Flow-Guided Sparse Transformer for Video Deblurring | [[Paper]](https://arxiv.org/abs/2201.01893)
* [2022/06] Deep Online Fused Video Stabilization | [[Paper]](https://openaccess.thecvf.com/content/WACV2022/papers/Shi_Deep_Online_Fused_Video_Stabilization_WACV_2022_paper.pdf)
* [2022/11] Real-Time Intermediate Flow Estimation for Video Frame Interpolation | [[Paper]](https://arxiv.org/abs/2011.06294)
* [2023/05] Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models | [[Paper]](https://arxiv.org/abs/2305.10474)
* [2023/06] Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation | [[Paper]](https://arxiv.org/abs/2306.07954)
* [2023/12] Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution | [[Paper]](https://arxiv.org/abs/2312.06640)
* [2024/01] Video Super-Resolution Transformer with Masked Inter&Intra-Frame Attention | [[Paper]](https://arxiv.org/abs/2401.06312)
* [2024/03] Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution | [[Paper]](https://arxiv.org/abs/2403.17000)
* [2024/07] Domain-adaptive Video Deblurring via Test-time Blurring | [[Paper]](https://arxiv.org/abs/2407.09059)

* [2024/08] Rethinking Video Deblurring with Wavelet-Aware Dynamic Transformer and Diffusion Model | [[Paper]](https://arxiv.org/abs/2408.13459)



### Training Strategies

* [2019/04] Spatio-Temporal Filter Adaptive Network for Video Deblurring | [[Paper]](https://arxiv.org/abs/1904.12257)
* [2022/09] Make-a-video: Text-to-video generation without text-video data | [[Paper]](https://arxiv.org/abs/2209.14792)
* [2022/10] Imagen video: High definition video generation with diffusion models | [[Paper]](https://arxiv.org/abs/2210.02303)
* [2023/03] Fatezero: Fusing attentions for zero-shot text-based video editing | [[Paper]](https://arxiv.org/abs/2303.09535)
* [2023/05] Control-A-Video: Controllable Text-to-Video Diffusion Models with Motion Prior and Reward Feedback Learning | [[Paper]](https://arxiv.org/abs/2305.13840)
* [2023/08] Modelscope text-to-video technical report  | [[Paper]](https://arxiv.org/abs/2308.06571)
* [2023/12] Diffusion Reward: Learning Rewards via Conditional Video Diffusion | [[Paper]](https://arxiv.org/abs/2312.14134)
* [2024/03] AnimateDiff-Lightning: Cross-Model Diffusion Distillation | [[Paper]](https://arxiv.org/abs/2403.12706)
* [2024/08] Cogvideox: Text-to-video diffusion models with an expert transformer | [[Paper]](https://arxiv.org/abs/2408.06072)
* [2024/08] xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations | [[Paper]](https://arxiv.org/abs/2408.12590)
* [2024/08] Rethinking Video Deblurring with Wavelet-Aware Dynamic Transformer and Diffusion Model | [[Paper]](https://arxiv.org/abs/2408.13459)
* [2024/12] Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation | [[Paper]](https://arxiv.org/abs/2412.21059)
* [2025/02] Goku: Flow Based Video Generative Foundation Models | [[Paper]](https://arxiv.org/abs/2502.04896)
* [2025/03] Rethinking video tokenization: A conditioned diffusion-based approach | [[Paper]](https://arxiv.org/abs/2503.03708)
* [2025/12] Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation | [[Paper]](https://arxiv.org/abs/2512.04678)



### Benchmarks and Evaluation Metrics

* [2023/08] StoryBench: A Multifaceted Benchmark for Continuous Story Visualization | [[Paper]](https://arxiv.org/abs/2308.11606)
* [2023/10] EvalCrafter: Benchmarking and Evaluating Large Video Generation Models | [[Paper]](https://arxiv.org/abs/2310.11440)
* [2023/11] VBench: Comprehensive Benchmark Suite for Video Generative Models | [[Paper]](https://arxiv.org/abs/2311.17982)
* [2024/01] AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI | [[Paper]](https://arxiv.org/abs/2401.01651)
* [2024/06] T2VBench: Benchmarking Temporal Dynamics for Text-to-Video Generation | [[Paper]](https://ieeexplore.ieee.org/document/10678444)
* [2024/06] ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation | [[[Paper]](https://arxiv.org/abs/2406.18522)
* [2024/07] MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions | [[[Paper]](https://arxiv.org/abs/2407.06358)
* [2024/07] Evaluation of Text-to-Video Generation Models: A Dynamics Perspective | [[[Paper]](https://arxiv.org/abs/2407.01094)
* [2024/11] VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models | [[[Paper]](https://arxiv.org/abs/2411.13503)
* [2025/03] VMBench: A Benchmark for Perception-Aligned Video Motion Generation | [[[Paper]](https://arxiv.org/abs/2503.10076)

